{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "#Suppressing all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def get_files(root):\n",
    "    return [os.path.join(root, f) for f in os.listdir(root)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = get_files(\"messages/inbox/\") + get_files(\"messages/archived_threads/\")\n",
    "users = []\n",
    "\n",
    "for f in folders:\n",
    "    user = f.split(\"/\")[-1]\n",
    "    if ord(user[0]) >= 65 and ord(user[0]) <= 90:\n",
    "        continue\n",
    "        \n",
    "    if \"files\" in os.listdir(f):\n",
    "        continue\n",
    "        \n",
    "    users.append(f)\n",
    "    \n",
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def fixup_str(text):\n",
    "    try:\n",
    "        return text.encode('latin1').decode('utf8')\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def fixup_list(l):\n",
    "    return [fixup(e) for e in l]\n",
    "\n",
    "\n",
    "def fixup_dict(dct):\n",
    "    return {fixup_str(k): fixup(v) for k, v in dct.items()}\n",
    "\n",
    "\n",
    "def fixup(e):\n",
    "    if isinstance(e, dict):\n",
    "        return fixup_dict(e)\n",
    "    if isinstance(e, list):\n",
    "        return fixup_list(e)\n",
    "    if isinstance(e, str):\n",
    "        return fixup_str(e)\n",
    "    return e\n",
    "\n",
    "\n",
    "def filter_messages(messages):\n",
    "    for message in messages:\n",
    "        if message['type'] == 'Generic':\n",
    "            yield message\n",
    "\n",
    "\n",
    "def format_message(message):\n",
    "#     print(message)\n",
    "    sender = message['sender_name']\n",
    "    timestamp = datetime.fromtimestamp(message['timestamp_ms']/1000)\n",
    "    try:\n",
    "        content = message['content']\n",
    "    except KeyError:\n",
    "        content = \"\"\n",
    "        \n",
    "    return f'{timestamp}\\t{sender}\\t{content}'\n",
    "\n",
    "\n",
    "def main(fname):\n",
    "    mess = []\n",
    "    with open(fname, 'r') as f:\n",
    "        data = fixup(json.loads(f.read()))\n",
    "        messages = data['messages']\n",
    "        participants = data[\"participants\"]\n",
    "        \n",
    "    for message in filter_messages(messages):\n",
    "        mess.append([len(participants), format_message(message)])\n",
    "        \n",
    "    return mess\n",
    "\n",
    "# json.load(open(\"messages/inbox/facebookuser_q1c23rfqfq/message_1.json\"), object_hook=parse_obj)\n",
    "main(\"messages/inbox/facebookuser_q1c23rfqfq/message_1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_data = []\n",
    "failed = []\n",
    "failed_file = []\n",
    "for u in users:\n",
    "    mess_files = get_files(u)    \n",
    "    for mf in mess_files:\n",
    "#         print(mf)\n",
    "        if not mf.endswith(\".json\"):\n",
    "            continue\n",
    "        try:\n",
    "            data = main(mf)\n",
    "        except:\n",
    "            failed_file.append(mf)\n",
    "            continue\n",
    "            \n",
    "        for lp, d in data:\n",
    "            try:\n",
    "                date, name, mess = d.split(\"\\t\")\n",
    "                df_data.append([date, u.split(\"/\")[-1], lp, name, mess])\n",
    "            except:\n",
    "                failed.append(d)\n",
    "            \n",
    "len(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df_data, columns=[\"DateTime\", \"Name\", \"NumPart\", \"UserName\", \"Message\"])\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=\"DateTime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_mess = {}\n",
    "for dt, _, lp, user, _ in df.to_numpy().tolist():\n",
    "    if int(lp) > 2:\n",
    "        continue\n",
    "    \n",
    "    if user not in direct_mess:\n",
    "        direct_mess[user] = 1\n",
    "    else:\n",
    "        direct_mess[user] += 1\n",
    "direct_mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_direct_mess = sorted(direct_mess.items(), key=lambda x: -x[1])\n",
    "sorted_direct_mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#CountPlot\n",
    "fig, ax = plt.subplots(figsize=(20, 15))\n",
    "ax.set_title('Number of direct messages', fontsize=20)\n",
    "sns.barplot(y=[s[0] for s in sorted_direct_mess[1:30]], x=[s[1] for s in sorted_direct_mess[1:30]], orient=\"h\")\n",
    "ax.set_xlabel('Messages', fontsize=15)\n",
    "ax.set_ylabel('Friends', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact = {}\n",
    "\n",
    "for dt, _, lp, user, _ in df.to_numpy().tolist():\n",
    "    if user not in interact:\n",
    "        interact[user] = 1 / lp\n",
    "    else:\n",
    "        interact[user] += 1 / lp\n",
    "        \n",
    "sorted_interact = sorted(interact.items(), key=lambda x: -x[1])\n",
    "sorted_interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountPlot\n",
    "fig, ax = plt.subplots(figsize=(20, 15))\n",
    "ax.set_title('Overall Messenger Interaction', fontsize=20)\n",
    "sns.barplot(y=[s[0] for s in sorted_interact[1:30]], x=[s[1] for s in sorted_interact[1:30]], orient=\"h\")\n",
    "ax.set_xlabel('Message score (groups included)', fontsize=15)\n",
    "ax.set_ylabel('Friends', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_date = {}\n",
    "\n",
    "for dt, _, lp, user, _ in df.to_numpy().tolist():\n",
    "    dt = int(dt.split(\" \")[0].replace(\"-\", \"\"))\n",
    "    if user not in interact_date:\n",
    "        interact_date[user] = 1 / lp * (dt - 20130225) ** 3\n",
    "    else:\n",
    "        interact_date[user] += 1 / lp * (dt - 20130225) ** 3\n",
    "        \n",
    "sorted_interact_date = sorted(interact_date.items(), key=lambda x: -x[1])\n",
    "sorted_interact_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountPlot\n",
    "fig, ax = plt.subplots(figsize=(20, 15))\n",
    "ax.set_title('Overall Messenger Interaction (prioritize recency)', fontsize=20)\n",
    "sns.barplot(y=[s[0] for s in sorted_interact_date[1:30]], x=[s[1] for s in sorted_interact_date[1:30]], orient=\"h\")\n",
    "ax.set_xlabel('Message score (groups included)', fontsize=15)\n",
    "ax.set_ylabel('Friends', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "df[\"Year\"] = df[\"DateTime\"].apply(lambda x: x.split(\" \")[0].split(\"-\")[0])\n",
    "hm_year = {}\n",
    "\n",
    "def get_heatmap_by_year(year):\n",
    "    df_me = df[(df[\"UserName\"] == \"Nguyễn Tài Long\") & (df[\"Year\"] == str(year))]\n",
    "    df_me[\"Hour\"] = df_me[\"DateTime\"].apply(lambda x: x.split(\" \")[1].split(\":\")[0])\n",
    "    df_me[\"Day\"] = df_me[\"DateTime\"].apply(lambda x: datetime.strptime(x.split(\".\")[0], \"%Y-%m-%d %H:%M:%S\").weekday())\n",
    "    tmp = df_me.groupby([\"Day\", \"Hour\"]).count().reset_index()[[\"Day\", \"Hour\", \"Message\"]]\n",
    "    hm = [[0 for _ in range(24)] for _ in range(7)]\n",
    "    for d, h, m in tmp.to_numpy().tolist():\n",
    "        hm[int(d)][int(h)] = m\n",
    "    \n",
    "    return hm\n",
    "\n",
    "for year in range(2013, 2021):\n",
    "    hm_year[year] = get_heatmap_by_year(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=8, ncols=1, figsize=(20, 5 * 8), dpi=200)\n",
    "for year in range(2013, 2021):    \n",
    "    axes[year - 2013].set_title('Usage Heat Map - {}'.format(year), fontsize=20)\n",
    "    sns.heatmap(data=hm_year[year], xticklabels=[i for i in range(24)], ax=axes[year - 2013], \n",
    "                yticklabels=[\"Mon\", \"Tue\", 'Wed', \"Thu\", \"Fri\", \"Sat\", \"Sun\"],\n",
    "               cmap=sns.light_palette(\"seagreen\", as_cmap=True))\n",
    "plt.show()\n",
    "plt.savefig(\"heatmaps.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_me = df[df[\"UserName\"] == \"Nguyễn Tài Long\"]\n",
    "\n",
    "all_my_text = df[\"Message\"].tolist()\n",
    "len(all_my_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "import re\n",
    "_CHARACTERS_DIGITS = 'abcdeghiklmnopqrstuvxyàáâãèéêìíòóôõùúýăđĩũơưạảấầẩẫậắằẳẵặẹẻẽếềểễệỉịọỏốồổỗộớờởỡợụủứừửữựỳỵỷỹ0123456789 '\n",
    "most_common_phrases = {}\n",
    "for text in all_my_text:\n",
    "    text = re.sub(\"[^{}]\".format(_CHARACTERS_DIGITS), \" \", text.lower())\n",
    "    for ph in ngrams(text.split(), 4):\n",
    "        ph = \" \".join(list(ph))\n",
    "        if ph not in most_common_phrases:\n",
    "            most_common_phrases[ph] = 1\n",
    "        else:\n",
    "            most_common_phrases[ph] += 1\n",
    "            \n",
    "    for ph in ngrams(text.split(), 3):\n",
    "        ph = \" \".join(list(ph))\n",
    "        if ph not in most_common_phrases:\n",
    "            most_common_phrases[ph] = 1\n",
    "        else:\n",
    "            most_common_phrases[ph] += 1\n",
    "            \n",
    "\n",
    "        \n",
    "sorted_phrases = sorted(most_common_phrases.items(), key=lambda x: -x[1])\n",
    "sorted_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "!pip install multidict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_phrases = [('nói chung là', 549),\n",
    " ('nói chuyện với', 388),\n",
    " ('là cái gì', 341),\n",
    " ('liên quan đến', 341),\n",
    " ('làm gì có', 338),\n",
    " ('ý t là', 319),\n",
    " ('ở đâu đấy', 293),\n",
    " ('đi đá bóng', 286),\n",
    " ('quan trọng là', 280),\n",
    " ('lúc nào cũng', 275),\n",
    " ('t nghĩ là', 264),\n",
    " ('có gì đâu', 255),\n",
    " ('liên quan gì', 253),\n",
    " ('hay sao ý', 252),\n",
    " ('ở công ty', 238),\n",
    " ('làm thế nào', 235),\n",
    " ('có vấn đề', 231),\n",
    " ('đang làm gì', 226),\n",
    " ('ko phải là', 226),\n",
    " ('như thế nào', 225),\n",
    " ('để làm gì', 220),\n",
    " ('mối quan hệ', 218),\n",
    " ('vấn đề là', 206),\n",
    " ('đang ở đâu', 204),\n",
    " ('có đứa nào', 201),\n",
    " ('thế nào rồi', 195),\n",
    " ('cái gì đấy', 194),\n",
    " ('đi xem phim', 189),\n",
    " ('xem thế nào', 184),\n",
    " ('đi xe máy', 182),\n",
    " ('em cảm ơn', 180),\n",
    " ('ở đâu thế', 178),\n",
    " ('vấn đề gì', 178),\n",
    " ('có thể là', 178),\n",
    " ('ko có gì', 177),\n",
    " ('t cũng k', 171),\n",
    " ('tất cả các', 169),\n",
    " ('đang ở nhà', 168),\n",
    " ('là thế nào', 168),\n",
    " ('làm gì đấy', 168),\n",
    " ('không có gì', 165),\n",
    " ('quan tâm đến', 165),\n",
    " ('mấy hôm nay', 163),\n",
    " ('cũng có thể', 163),\n",
    " ('có thời gian', 162),\n",
    " ('mình có thể', 160),\n",
    " ('đi chơi với', 159),\n",
    " ('cuối tuần này', 158),\n",
    " ('làm cái gì', 155),\n",
    " ('lên công ty', 155),\n",
    " ('ở infore', 154),\n",
    " ('em có thể', 153),\n",
    " ('gì đâu mà', 153),\n",
    " ('chứ k phải', 149),\n",
    " ('có cơ hội', 147),\n",
    " ('có thể làm', 145),\n",
    " ('câu trả lời', 142),\n",
    " ('hình như là', 142),\n",
    " ('về đến nhà', 141),\n",
    " ('k đi đc', 137),\n",
    " ('thực ra là', 136),\n",
    " ('cảm ơn anh', 136),\n",
    " ('tóm lại là', 135),\n",
    " ('mừng sinh nhật', 134),\n",
    " ('đứa con gái', 134),\n",
    " ('kiểu gì cũng', 133),\n",
    " ('thôi ngủ đi', 133),\n",
    " ('cái này thì', 132),\n",
    " ('thế này thì', 132),\n",
    " ('bọn con gái', 128),\n",
    " ('có khả năng', 127),\n",
    " ('hôm nay đi', 126),\n",
    " ('anh lê công thành', 125),\n",
    " ('dạ vâng ạ', 124),\n",
    " ('chúc mừng sinh nhật', 124),\n",
    " ('cho các bạn', 124),\n",
    " ('công ty mình', 123),\n",
    " ('hạnh má mì', 123),\n",
    " ('cho em hỏi', 122),\n",
    " ('có cái gì', 121),\n",
    " ('mấy cái này', 121),\n",
    " ('t cũng thế', 121),\n",
    " ('nào cũng đc', 120),\n",
    " ('t ko biết', 120),\n",
    " ('có chuyện gì', 118),\n",
    " ('sao tự nhiên', 118),\n",
    " ('đi ngủ đây', 117),\n",
    " ('có kinh nghiệm', 117),\n",
    " ('đi du lịch', 116),\n",
    " ('ý em là', 116),\n",
    " ('chuẩn bị đi', 113),\n",
    " ('có gì hot', 113),\n",
    " ('làm gì mà', 112),\n",
    " ('xử lý ảnh', 112),\n",
    " ('mai mấy h', 110),\n",
    " ('cái gì đó', 110),\n",
    " ('bình thường mà', 109),\n",
    " ('chả có gì', 109),\n",
    " ('t đang ở', 108),\n",
    " ('thế nào là', 108),\n",
    " ('t cũng ko', 108),\n",
    " ('phải đi học', 108),\n",
    " ('đi du học', 108),\n",
    " ('làm gì đâu', 107),\n",
    " ('có mấy đứa', 107),\n",
    " ('đến đâu rồi', 107),\n",
    " ('có ai đi', 107),\n",
    " ('có gì mà', 106),\n",
    " ('cái đấy thì', 106),\n",
    " ('t đi học', 106),\n",
    " ('phụ thuộc vào', 105),\n",
    " ('nào cũng được', 105),\n",
    " ('xem bóng đá', 105),\n",
    " ('em nghĩ là', 104),\n",
    " ('tuần sau đi', 104),\n",
    " ('mới bắt đầu', 104),\n",
    " ('được không ạ', 104),\n",
    " ('có vấn đề gì', 103),\n",
    " ('chứ không phải', 103),\n",
    " ('không phải là', 103),\n",
    " ('thì t cũng', 103),\n",
    " ('ví dụ như', 102),\n",
    " ('làm gì thế', 101),\n",
    " ('cũng k biết', 101),\n",
    " ('k có gì', 101),\n",
    " ('thì nó sẽ', 101),\n",
    " ('thôi ngủ đây', 101),\n",
    " ('các anh chị', 101),\n",
    " ('thế nào ạ', 100),\n",
    " ('thế còn gì', 100),\n",
    " ('đi uống bia', 75)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multidict as multidict\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from PIL import Image\n",
    "from scipy.ndimage import gaussian_gradient_magnitude\n",
    "\n",
    "fullTermsDict = multidict.MultiDict()\n",
    "    \n",
    "for k, v in interesting_phrases:\n",
    "    fullTermsDict.add(k, v)\n",
    "\n",
    "# load image. This has been modified in gimp to be brighter and have more saturation.\n",
    "parrot_color = np.array(Image.open(\"parrot-by-jose-mari-gimenez2.jpg\"))\n",
    "# subsample by factor of 3. Very lossy but for a wordcloud we don't really care.\n",
    "parrot_color = parrot_color[::3, ::3]\n",
    "\n",
    "# create mask  white is \"masked out\"\n",
    "parrot_mask = parrot_color.copy()\n",
    "parrot_mask[parrot_mask.sum(axis=2) == 0] = 255\n",
    "\n",
    "# some finesse: we enforce boundaries between colors so they get less washed out.\n",
    "# For that we do some edge detection in the image\n",
    "edges = np.mean([gaussian_gradient_magnitude(parrot_color[:, :, i] / 255., 2) for i in range(3)], axis=0)\n",
    "parrot_mask[edges > .08] = 255\n",
    "\n",
    "# wc = WordCloud(background_color=\"black\", max_words=1000, width=3000, height=2000)\n",
    "wc = WordCloud(max_words=2000, mask=parrot_mask, max_font_size=40, random_state=42, relative_scaling=0, scale=1.5)\n",
    "# generate word cloud\n",
    "wc.generate_from_frequencies(fullTermsDict)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 15))\n",
    "ax.imshow(wc, interpolation=\"bilinear\")\n",
    "ax.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# create coloring from image\n",
    "image_colors = ImageColorGenerator(parrot_color)\n",
    "wc.recolor(color_func=image_colors)\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(wc, interpolation=\"bilinear\")\n",
    "wc.to_file(\"parrot_new.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
